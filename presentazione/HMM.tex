% $Header: /cvsroot/latex-beamer/latex-beamer/solutions/generic-talks/generic-ornate-15min-45min.en.tex,v 1.5 2007/01/28 20:48:23 tantau Exp $

\documentclass[mathserif]{beamer}
%\documentclass{beamer}


% This file is a solution template for:

% - Giving a talk on some subject.
% - The talk is between 15min and 45min long.
% - Style is ornate.



% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 


\mode<presentation>{
  \usetheme{Copenhagen}
  % or ...
  

  

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}
%\usecolortheme{whale}

\usepackage[italian]{babel}
% or whatever

\usepackage[latin1]{inputenc}
% or whatever

\usepackage{times}
%\usepackage{cite}
\usepackage[T1]{fontenc}
%\usepackage{fourier}
\usepackage{eulervm}
\usepackage{algorithm}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.


\title[Elaborato di Apprendimento Automatico] % (optional, use only with long paper titles)
{}

\subtitle
{Accelerometer Based Gesture Recognition using HMMs} % (optional)

\author[Andrea~Tarocchi, Marco~Magnatti] % (optional, use only with lots of authors)
{Andrea~Tarocchi, Marco~Magnatti}
% - Use the \inst{?} command only if the authors have different
%   affiliation.


\date[Short Occasion] % (optional)
{8 gennaio 2009}

\subject{Talks}
% This is only inserted into the PDF information catalog. Can be left
% out. 



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

 \pgfdeclareimage[height=0.7cm]{university-logo}{university-logo-filename}
 \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

%\beamerdefaultoverlayspecification{<+->}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}


% Since this a solution template for a generic talk, very little can
% be said about how it should be structured. However, the talk length
% of between 15min and 45min and the theme suggest that you stick to
% the following rules:  

% - Exactly two or three sections (other than the summary).
% - At *most* three subsections per section.
% - Talk about 30s to 2min per frame. So there should be between about
%   15 and 30 frames, all told.

\section{Introduzione}

\begin{frame}{Descrizione del problema}{}
  % - A title should summarize the slide in an understandable fashion
  %   for anyone how does not follow everything on the slide itself.

\begin{block}{}
Da un device dotato di accelerometri si ricevono i dati relativi alle accelerazioni a cui \`e sottoposto durante il moto (lungo le tre coordinate spaziali).
\end{block}
\begin{block}{}
Tale device pu\`o essere impugnato per eseguire gesture.
\end{block}
\begin{block}{}
Il nostro obietivo \`e realizzare un sistema che, dopo adeguato addestramento, sia in grado di riconoscere gesti effettuati tramite tale dispositivo.
\end{block}

\end{frame}


\begin{frame}{Strumenti utilizzati}

\begin{columns}[c]

\begin{column}[T]{2cm}

 \begin{figure}[htbp]
 	\centering
	\includegraphics[scale=0.42]{wii.png}
	\label{fig:wii}
 \end{figure}

\end{column}

\begin{column}[t]{9cm}

  \begin{block}{}
	\begin{itemize}
 		\item Device di acquisizione dati: Nintendo Wiimote
		\item Linguaggio di programmazione utilizzato: C/C++
		\item wiiuse: libreria C per comunicazione PC - Wiimote
		\item boost C++: librerie C++ con numerose strutture dati
		\item HMM (Hidden Markov Model): per il riconoscimento
	\end{itemize}
  \end{block}

\end{column}
 
\end{columns}

\end{frame}

\begin{frame}{Struttura generale}

\begin{figure}[htbp]
	\centering
		\includegraphics[scale=0.5]{schema.png}
	\label{fig:schema}
\end{figure}

\end{frame}

\section{Quantizzazione dei dati}

\begin{frame}{Quantizzazione con k-means}
\begin{block}{}
Poich\'e i dati in arrivo dal Wiimote sono valori in campo reale compresi tra -4G e +4G, \`e necessaria una discretizzazione prima di poterli sottoporre ad un HMM che, per scelta implementativa, lavora in campo discreto.
\end{block}

\begin{block}{}
La discretizzazione viene fatta tramite l'algoritmo k-means.
\end{block}

\begin{block}{}
Per via sperimentale, si \`e stabilito la posizione iniziale e il numero dei centroidi da utilizzare (e di conseguenza il numero di differenti simboli che l'HMM pu\`o emettere).
\end{block}

\end{frame}

\begin{frame}{Posizione dei centroidi}

\begin{columns}[c]

\begin{column}[T]{7cm}

 \begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.35]{centroidi.png}
	\label{fig:centroidi}
 \end{figure}

\end{column}

\begin{column}[t]{5cm}

 \begin{block}{}
	Possibili approcci alternativi:
	\begin{itemize}
		\item Inizializzazione random
		\item Inizializzazione lungo la direzione del vettore medio
	\end{itemize}

 \end{block}

\end{column}
 
\end{columns}

\end{frame}


\section{HMM}

\subsection{Definizione}

\begin{frame}
 \begin{block}{}
  Con HMM \`e possibile affrontare problemi del tipo ``modellare la probabilit\`a $P(X)$ di una sequenza di osservazioni $X$''.\\
Nel nostro caso, le sequenze osservate sono costituite dai vettori di accelerazioni quantizzati in un numero finito di simboli.
 \end{block}

 \begin{block}{}
  Si assume che la sequenza osservata sia generata da una sequenza $S$ di variabili nascoste, chiamate stati.
 \end{block}

 \begin{block}{}
  Altra assunzione fondamentale \`e che ``il futuro \`e indipendente dal passato, dato il presente'', ovvero vale: \\
$S[t+1] \perp S[1], S[2], \ldots, S[t-1], X[1], X[2], \ldots, X[t] | S[t]$ \\
$X[t+1] \perp S[1], S[2], \ldots, S[t], X[1], X[2], \ldots, X[t] | S[t+1]$
 \end{block}
\end{frame}


\subsection{Riconoscimento}

\begin{frame}
 \begin{block}{}
  blah
 \end{block}

\end{frame}


\subsection{Apprendimento}

\begin{frame}
 \begin{block}{}
  blah
 \end{block}

\end{frame}
\subsection{Decodifica}

\begin{frame}{La procedura Concave-Convex(CCCP)}
Il problema (3) non � convesso. CCCP � una procedura per ottimizzare problemi non convessi. 
Assumiamo che una funzione di costo $J(\Theta)$ possa essere riscritta come la somma di una parte convessa $J_{vex}(\Theta)$ e di una parte concava $J_{cav}(\Theta)$. Ogni iterazione della procedura CCCP approssima la parte concava con la sua tangente e minimizza la funzione convessa risultante.

\begin{equation}
	\Theta^{t+1}=\underset{\Theta}{\operatorname{arg\,min}}(J_{vex}(\Theta)+J_{cav}^{\prime}(\Theta^t)\cdot \Theta).
\end{equation}

\end{frame}


\begin{frame}{CCCP-2}
\begin{block}<1->{Algoritmo Iterativo}
%\begin{figure}[htbp]
%	\centering
%		\includegraphics[scale=0.35]{CCCP_procedure.png}
%	\label{fig:gres}
%\end{figure}
\end{block}


\begin{block}<1-> {}
Si pu� dimostrare che:
\begin{itemize}
	\item<1->  $J(\Theta^t)$ decresce ad ogni iterazione;
	\item<1-> la procedura converge ad un minimo locale.
\end{itemize}
\end{block}
\end{frame}


\begin{frame}{CCCP per TSVMs}
\begin{block}{}
La Ramp Loss pu� essere riscritta come differenza tra due Hinge Loss
\begin{equation}
	R_s(z)=H_1(z)-H_s(z)
\end{equation}
\'E quindi possibile applicare la procedura CCCP al problema di minimizzazione TSVM (3).Il costo $J^s(\Theta)$ pu� essere decomposto in una parte convessa $J^s_{vex}(\Theta)$ e in una parte concava $J^s_{cav}(\Theta)$
\end{block}
\end{frame}

\begin{frame}{CCCP per TSVMs-2}
\begin{block}{}
\begin{equation}
\begin{split}
J^s(\Theta)& =\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^{L}H_1(y_if_\Theta(\textbf{x}_i))+C^*\sum_{i=L+1}^{L+2U}R_s(y_if_\Theta(\textbf{x}_i))=\\
 & =\underbrace{\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^{L}H_1(y_if_\Theta(\textbf{x}_i))+C^*\sum_{i=L+1}^{L+2U}H_1(y_if_\Theta(\textbf{x}_i))-}_{J^s_{vex}(\Theta)}\\
 &\underbrace{-C^*\sum_{i=L+1}^{L+2U}H_s(y_if_\Theta(\textbf{x}_i))}_{J^s_{cav}(\Theta)}.
\end{split}
\end{equation}
\end{block}
\end{frame}

\begin{frame}{CCCP per TSVMs-3}
\begin{block}{}
La decomposizione precedente permette di applicare la procedura CCCP. Il problema convesso alla base della procedura viene espresso utilizzando le variabili duali $\boldsymbol\alpha$. Dobbiamo anche implementare il vincolo di bilanciamento (4). 
Per semplificare l'approssimazione del primo ordine della parte concava, denotiamo:
\begin{equation}
\beta_i\ \  =\ \ 	\frac{\partial J_{cav}^{s}(\Theta)}{\partial f_{\Theta}(x_i)} \ \  = \ \ 
\begin{cases} 
C^* & \text{if } y_if_\Theta(\textbf{x}_i) \\
0 & \text{altrimenti }
\end{cases},
\end{equation}
per gli esempi non etichettati ($\beta_i=0$ per $i \leq L$).
\end{block}
\end{frame}


\begin{frame}{CCCP per TSVMs-4}
%\begin{figure}[htbp]
%	\centering
%		\includegraphics[scale=0.32]{alg2.png}
%	\label{fig:gres}
%\end{figure}
\end{frame}

\begin{frame}{Complessit\`a}
\begin{block}{}
Addestrare con la procedura CCCP-TSVM richiede la risoluzione di una serie di problemi di ottimizzazione SVM con ($L+2U$) variabili. Sebbene SVM abbia complessit� $ \mathcal{O} (L+2U)^3)$ nel caso peggiore, nel caso tipico scala quadraticamente e lo stesso vale per i sottoproblemi di TSVMs. Assumendo un numero costante di iterazioni TSVMs con CCCP dovrebbero scalare quadraticamente nella maggior parte dei casi.
\end{block}
\end{frame}


\section{Decisore}

\begin{frame}{Esperimenti eseguiti}
\begin{block}{}
Si sono condotti esperimenti sul database MNIST trattato come un problema a 10-classi. Il dataset ha 42k esempi di training, 18k di validation e 10k di test. Il training set è stato campionato per ottenere il punti etichettati ed � stato usato il training set per gli esempi non etichettati. Abbiamo replicato il set-up del paper ovvero:
\begin{itemize}
	\item <1-> $C=10$ e $\gamma = 0.0128$ con kernel RBF;
	\item <1-> $C^*$ variabile con il numero di esempi non etichettati e $s=0.01$.
\end{itemize}
L'errore � stato valutato sul validation test.
\end{block}
\end{frame}

\begin{frame}
%\begin{figure}[htbp]
%	\centering
%		\includegraphics[scale=1.]{result.png}
%	\caption{Andamento dell'errore al variare del numero di dati non etichettati. Con $0$ dati non etichettati si intende SVM induttivo}
%	\label{fig:gres}
%\end{figure}
\end{frame}

\begin{frame}
%\begin{figure}[htbp]
%	\centering
%		\includegraphics[scale=1.]{tempo.png}
%	\caption{Andamento del tempo di ottimizzazione al variare del numero di dati non etichettati.}
%	\label{fig:gres}
%\end{figure}
\end{frame}


\begin{frame}{Commento dei risultati}
\begin{block}{}
Dal grafico dell'errore si nota che l'\alert{aggiunta di dati non etichettati porta ad un significativo miglioramento delle prestazioni}. A meno di uno scarto medio dell' $1,71\%$ l'andamento dell'errore degli esperimenti effettuati � lo stesso di quelli riportati nel paper. Dal grafico dei tempi di vede chiaramente che il tempo di ottimizzazione ha un andamento \alert{quadratico} con il numero dei dati non etichettati confermando le ipotesi fatte in precedenza.
\end{block}
\end{frame}

\begin{frame}{SSL Vs Transductive Learning}
\begin{block}{}
Se si considera l'apprendimento trasduttivo come una versione di SSL dove i dati non etichettati sono il test set di interesse, � possibile comparare le prestazioni dei due approcci. L'esperimento prevede l'utilizzo di dati non etichettati diversi dal test set per l'SSL. Nello specifico abbiamo utilizzato 10000 punti non etichettati, un test set di 10000 punti e un training set di 1000 punti. I risultati ottenuti:
\begin{itemize}
	\item<1-> Testing error SSL: \alert{$6,944 \%$};
	\item<1-> Testing error Transductive: \alert{$6,350 \%$}.
\end{itemize}
\end{block}
\end{frame}

\section{Test e risultati}


\begin{frame}{Considerazioni}
\begin{block}{}
\begin{itemize}
	\item<1-> Si � verificato che l'utilizzo di dati non etichettati migliora le prestazioni di un classificatore induttivo.
	\item<1-> Comunque il trade-off tra costi computazionali e benefici non � soddisfacente.
	\item<1-> Dal momento che l'algoritmo ottimizza problemi SVMs standard, ogni miglioramento delle tecniche di risoluzione di questi si riflette immediatamente sulle TSVMs.  
	\item<1-> Interessante l'approccio trasduttivo in particolari situazioni.
\end{itemize}
\end{block}
\end{frame}




\section{Bibliografia}

\begin{frame}
	\frametitle{Bibliografia}
	
		\begin{thebibliography}{}
		%
%			\bibitem<1->[Solomaa, 1973]{Solomaa1973}
%				A.~Salomaa.
%				\newblock {\em Formal Languages}.
%				\newblock Academic Press, 1973.
				\footnotesize
				
				\bibitem<1->[Solomaa, 1973]{Solomaa1973}
			 	Collobert R. and Sinz F. and et al.  
				\newblock {\em Large Scale Transductive SVMs}.
				
				\bibitem<1->[Solomaa, 1973]{Solomaa1973}
				Schlkopf, Bernhard   and Smola, Alexander  J. 
				\newblock {\em Learning with Kernels: Support Vector Machines, Regularization, 		 Optimization, and Beyond (Adaptive Computation and Machine Learning).}
				\newblock The MIT Press, 2001.
				
				\bibitem<1->[Solomaa, 1973]{Solomaa1973}
				Chapelle, O.  and Sch\"{o}lkopf, B.  and Zien, A. 
				\newblock {\em Semi-Supervised Learning (Adaptive Computation and Machine Learning)}.
				\newblock The MIT Press, 2006.


\end{thebibliography}
\end{frame}

\begin{frame}
	\frametitle{Bibliografia}
	
		\begin{thebibliography}{}
				\footnotesize
				\bibitem<1->[Solomaa, 1973]{Solomaa1973}
				Yuille A. L. and Rangarajan A. 
				\newblock {\em The concave-convex procedure (CCCP)}.
				\newblock The MIT Press, 2002.
				
				
				\bibitem<1->[Solomaa, 1973]{Solomaa1973}
				Vapnik, Vladimir  N.  
				\newblock {\em The Nature of Statistical Learning Theory}.
				\newblock Springer, 1999.
				
				
					
				\bibitem<1->[Solomaa, 1973]{Solomaa1973}
			   N. Cristianini and J. Shawe-Taylor.
				\newblock {\em Kernel Methods for Pattern 
Analysis}.
				\newblock Cambridge Univ. Press, 2004.
				
				
				
				
				
			
				

\end{thebibliography}
\end{frame}

\end{document}

